# Vectorization
For computer, doing Loop to handling some iterating tasks is slow. Store some information in vectors, matrices, tensors can be greatly accelerated.
# Derivative for Matrix
## Gradient for Vector
Given a function with $1$ output and $n$ inputs(n-dimension vector), we have
$$
f(x)=f(x_1,x_2,......,x_n)
$$
then
$$
\frac{\partial f}{\partial x}=[\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2},\frac{\partial f}{\partial x_3},......,\frac{\partial f}{\partial x_n}]
$$
## Jacobian Matrix
The gradient is for a function that inputs $n$ and outputs $1$.
The Jacobian Matrix is for a function that inputs $n$ and outputs $m$.
We have
$$
\begin{matrix}f(x)=[f_1(x_1,x_2,......,x_n),......,f_m(x_1,x_2,......,x_n)]\\=[f_1(x),f_2(x),......,f_m(x)]\end{matrix}
$$
then
$$
\frac{\partial f}{\partial x}=\begin{bmatrix}\frac{\partial f_1}{\partial x_1}......\frac{\partial f_1}{\partial x_n}\\............\\\frac{\partial f_m}{\partial x_1}......\frac{\partial f_m}{\partial x_n}\end{bmatrix}
$$
### Elementwise Activation Function
For a activation function $h(x)$ which takes $n$ input and $n$ output, the Jacobian Matrix should be a diagonal $n \times n$ matrix.
$$
(\frac{\partial h}{\partial z})_{ij}=\begin{cases}h'(z_i)\,\,if\,\,\,i=j\\0\,\,\,else\end{cases}
$$
### Examples for Jacobian
$$
\frac{\partial(Wx+b)}{\partial x}=W
$$
$$
\frac{\partial(Wx+b)}{\partial b}=E(Identity Matrix)
$$
$$
\frac{\partial(u^Th)}{\partial u}=h^T
$$
## Shape Convention
$s$ a single value while $W$ is $n \times m$ matrix.
We define that $\frac{\partial s}{\partial W}$ is also a $n\times m$ matrix.
By convention, we set the gradient of any object to be the shape of that object.
# Appliance of Chain Rule
## The Chain Rule
For a neural network with a linear layer, activation layer, and a last linear layer, we have
$x(input)$,$z=Wx+b$,$h=f(z)$,$s=u^Th$.
In terms of finding $\frac{\partial s}{\partial b}$, we should work out
$$
\frac{\partial s}{\partial b}=\frac{\partial s}{\partial h}\cdot\frac{\partial h}{\partial z}\cdot\frac{\partial z}{\partial b}
$$
We then have
$$
\frac{\partial s}{\partial b}=u^T \text{diag}(f'(z))I=u^T \odot f'(z)
$$
The symbol $\odot$ means element-wise multiplication between two same-dimensional vectors.
## Reusing Computation
For $\frac{\partial s}{\partial W}$, we have
$$
\frac{\partial s}{\partial W}=\frac{\partial s}{\partial h}\cdot\frac{\partial h}{\partial z}\cdot\frac{\partial z}{\partial W}
$$
So, $\frac{\partial s}{\partial W}$ and $\frac{\partial s}{\partial b}$ have same part $\frac{\partial s}{\partial h}\cdot\frac{\partial h}{\partial z}$.
In regard to they are in a same layer, so the value can be passed from upstream layers and be reused to work out target derivatives.
### Dimension Analyse
$W$ is $n\times m$; $x$ is $m\times 1$; $b,z,h,u$ is $n\times 1$.
$\frac{\partial s}{\partial h}\cdot\frac{\partial h}{\partial z}$ is $1\times n$.
$\frac{\partial z}{\partial W}$ is $n\times n\times m$.
Them multiplies, the result is $1\times n\times m$, can collapses to $n\times m$, in this cases, meet the definition of the Shape Convention.
# Numeric Gradient
Numeric Gradient is used to check whether the algorithm to work out gradients is implemented correctly.
Starting from the definition of derivative, we see both sides of a certain value.
(for small $h \approx$ `1e-4` on computer)
$$
f'(x)\approx\frac{f(x+h)-f(x-h)}{2h}
$$
# Computation Graph
While doing forward propagation and backward propagation, we should have a graph to store our chain of calculation.
![[截屏2025-10-18 20.39.32.png]]
When backward propagation, just go through a reverse order of the graph carrying the current accumulated derivatives.
![[截屏2025-10-18 20.41.30.png]]
For a single Node case:
![[截屏2025-10-18 20.41.59.png]]
![[截屏2025-10-18 20.43.25.png]]
If the route divides in a node, then the gradient back passed from all routes should be sum up.
![[截屏2025-10-18 20.45.15.png]]
So in theory, we can build networks not only have classical neurons linked regularly layer by layer, but also like a **DAG** with diverse type of nodes.
Just work out gradient for each node. And go through the graph based on a **topological order** of the graph.
![[截屏2025-10-18 20.48.06.png]]
# Adam
Adam is a optimizing method that introduces **Momentum $m$** and **Adaptive Learning Rate $v$**.
Given the total parameter set $\theta$ and the hyper parameter $\alpha,\beta_1,\beta_2$, for the learning batch $t$, we modify
![[截屏2025-10-21 16.08.40.png]]
## Advantage of momentum
*Generated by Doubao*
$m$ is a rolling average of gradients. Instead of relying solely on the noisy gradient of the current minibatch, it combines past gradients. This smooths out the fluctuations in gradients, so parameter updates don't swing wildly. With lower variance, updates are more stable and consistent, which helps the model converge better and learn more reliably, avoiding being thrown off by the randomness in small batches of data.
## Advantage of Adaptive Learning Rate
$v$ is a kind of rolling average of the magnitude(模长) of every gradient.
*Generated by Doubao*
Some parameters might have only had small gradient signals so far (leading to small $v$). By giving these parameters larger updates, Adam ensures they can still adjust significantly and contribute to learning. This way, even parameters that haven't been changing much get a chance to be updated meaningfully, helping the model learn more comprehensively and efficiently.
# Dropout
Dropout is a regularization technique.
For a hidden layer $h$ in the model, to every node, we have possibility $p$ to ignore it.
For ignored nodes, they have no output in forward propagation and no update to themselves in backward propagation.
If the some nodes are ignored in a layer, the total output of the layer may be smaller(the output signal may be weaker).
So, to every surviving node, we multiply $\frac{1}{1-p}$ to its output.
$$
h_{modified\,\,out}=\frac{1}{1-p}h_{origin\,\,out}
$$
We can use vector multiplication to make process faster instead of using for-loop.
## Circumstances to Apply Dropout
Dropout should only be applied while training.
When the model is used to predict, we should not drop out any nodes to throughly use the information stored in the neural network.